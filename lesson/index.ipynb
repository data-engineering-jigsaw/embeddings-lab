{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f229c66-0269-40d1-9ac5-c2a9162c2dcb",
   "metadata": {},
   "source": [
    "# Embeddings Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed9cbc-4986-495d-bf8f-3dddff730296",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382ea61-239a-4198-8f19-117055e9c513",
   "metadata": {},
   "source": [
    "In this lesson, we'll work with a dataset of restaurant reviews to build a chatbot that can give restaurant recommendations.  \n",
    "\n",
    "We'll do this in two parts.  \n",
    "\n",
    "1. Build the database\n",
    "\n",
    "First, we'll read our original reviews, and retrieve an embedding representation for each one, and then store the text along with the correspodning embedding vector in a parquet file.\n",
    "\n",
    "2. Finding the related reviews\n",
    "Once we have our reviews along with the corresponding embedding, we can take a question, embed it, and then use cosine similarity to find the reviews most relevant to our question.  From there, we'll combine these reviews and provide them as a context to our llm model.  \n",
    "\n",
    "Ok, let's get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2fda4-e494-4b4f-b645-6a94b3797f99",
   "metadata": {},
   "source": [
    "### Building our database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceef824-44d8-47bc-9dba-2496ede9840a",
   "metadata": {},
   "source": [
    "You can get an overview of how we will build our database, by looking at the `db_builder/build_db.py` file.\n",
    "\n",
    "```python\n",
    "file_name = './source_data/reviews.csv'\n",
    "df = read_csv(file_name)\n",
    "text_series = combine_data(df)\n",
    "combined_embedded = build_in_batches_from(text_series, batch_size = 30)\n",
    "combined_embedded.to_parquet('../database.parquet')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40474600-038e-425b-9f68-54312831b251",
   "metadata": {},
   "source": [
    "As you can see, we'll read in our original reviews, then we'll combine the `Review` as well as the `Restaurant` columns to create one combined column. \n",
    "\n",
    "Then the build_in_batches function is what will turn each `restaurant_name` + `review` into an embedding vector.  We'll return a dataframe that has both the original review and the corresponding embedding and save this to a parquet file.\n",
    "\n",
    "Ok, let's take these functions in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6556b07-8b63-4df6-b57a-e40d4042cf6e",
   "metadata": {},
   "source": [
    "1. `combined_data`\n",
    "\n",
    "* Take in a dataframe of the reviews, and return a series where each entry returns the string with both the restaurant and the review.  For example if we had a restaurant of `Chipotle` and a review of `good tacos` the entry would be:\n",
    "\n",
    "`'restaurant name: Chipotle Review: good tacos'`\n",
    "\n",
    "> See the corresponding test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f46493-c4c1-432b-a364-cc1e21d7349c",
   "metadata": {},
   "source": [
    "2. `text_to_vectors`\n",
    "\n",
    "This function takes in a list of strings and returns a corresponding numpy array as the embedded vector for each string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31162417-48a7-44df-a27d-f88798ccf7b4",
   "metadata": {},
   "source": [
    "3. `build_embeddings_df_from(text_inputs)`\n",
    "\n",
    "* This takes the `text_inputs` strings and returns a dataframe that has one column of the reviews and another column of the corresponding numpy array.   Use the `text_to_vectors` function to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98dbadd-568d-4d2e-a056-5f740db5e502",
   "metadata": {},
   "source": [
    "4. `build_in_batches_from(combined_series, batch_size)`\n",
    "\n",
    "Now there are two problems with our build_embedding_df_from(text_inputs).  \n",
    "\n",
    "1. One is that we cannot feed all of our data at once to the embeddings API.  So instead we need to do this in batches.  For us that means 30 records at a time.  So use the `range()` to loop through and select 30 records at a time, and feed those records to the `build_embeddings_from` function to return a list of dataframes of the text and embeddings.\n",
    "\n",
    "Finally, combine all of the dataframes by using the pandas `concat` function.\n",
    "\n",
    "2. Another problem is that we need to make sure we only feed text to the embedding api.  And we still have some NAN values in our series.  So make sure the function does some preprocessing by removing all of the na values before feeding them to the embedding api.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aac5fb-4035-49b9-824e-d3220f6d8a92",
   "metadata": {},
   "source": [
    "* Set it up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada698b7-b544-4ed4-96ab-0225cb7a8989",
   "metadata": {},
   "source": [
    "Now you can `cd` into the `db_builder` folder and run the build_db.py file.  \n",
    "\n",
    "`python3 build_db.py`\n",
    "\n",
    "Notice that we save the data as a parquet file.  We need to do that, because we need to save our numpy array data.  If we stored it as a csv file, it would not be saved as a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435af9f-fd65-4faa-bfb8-2e2ef84f22e6",
   "metadata": {},
   "source": [
    "### LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d6001-8294-450c-ae7c-a5a85d144d52",
   "metadata": {},
   "source": [
    "* `build_context_from_distances_to(question, db_path)`\n",
    "\n",
    "    * This takes in arguments of the question, and the path to the database we just created.  It will return the three most relevant reviews as the context to pass to the llm.\n",
    "    * Each review should be separated by two new line characters '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221fa21-d32c-4e05-a40c-48411dbc5391",
   "metadata": {},
   "source": [
    "* `generate_prompt`\n",
    "    * This takes the question and context, to return corresponding instructions to be fed to our llm model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472752e-3cc0-4b0a-b6a5-3de323713bbf",
   "metadata": {},
   "source": [
    "* `question_and_answer(question, db_path)`\n",
    "    * This uses the above functions in the file to take in a question and return the corresponding answer from the llm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce140229-0cfa-4586-9e5a-f33de03cb1b2",
   "metadata": {},
   "source": [
    "When these functions are complete, look at the `llm_runner.py` file.  It uses the `question_and_answer` function to take in a question, and return a corresponding answer based on the information in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c8bcb-df0c-4dd3-bbcf-83701dae3f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
